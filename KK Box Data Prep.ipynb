{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "This jupyter notebook performs data preparation on KK-Box data, supplied for the following Kaggle competition:\n",
    "https://www.kaggle.com/c/kkbox-churn-prediction-challenge\n",
    "\n",
    ">\"In this challenge, you are asked to predict whether a user will churn after his/her subscription expires. Specifically, we want to forecast if a user make a new service subscription transaction within 30 days after the current membership expiration date.\n",
    ">\n",
    ">KKBOX offers subscription based music streaming service. When users signs up for our service, users can choose to either manual renew or auto-renew the service. Users can actively cancel their membership at any time.\n",
    ">\n",
    ">The churn/renewal definition can be tricky due to KKBox's subscription model. Since the majority of KKBox's subscription length is 30 days, a lot of users re-subscribe every month. The key fields to determine churn/renewal are transaction date, membership expiration date, and is_cancel. Note that the is_cancel field indicates whether a user actively cancels a subscription. Subscription cancellation does not imply the user has churned. A user may cancel service subscription due to change of service plans or other reasons. The criteria of \"churn\" is no new valid service subscription within 30 days after the current membership expires.\"\n",
    "\n",
    "In this script I take some inspiration from other users submissions, as the main focus of this project is not data preparation/wrangling but rather feature contribution extraction from the GBT model (in the “LightGBM – Feature Relationships” script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import time\n",
    "\n",
    "path = \"C:/Users/andrew.davidson/OneDrive - Concentra Consulting Limited/Documents/Projects/GBT Feature Relationships/KK Box/data/churn_comp_refresh\"\n",
    "train = pd.read_csv(path+'/train_v2.csv')\n",
    "sample_submission = pd.read_csv(path+'/sample_submission_v2.csv')\n",
    "transactions = pd.read_csv(path+'/transactions_v2.csv')\n",
    "user_logs = pd.read_csv(path+'/user_logs_v2.csv')\n",
    "members = pd.read_csv(path+'/members_v3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.25971221923828  MB\n",
      "34.118035316467285  MB\n"
     ]
    }
   ],
   "source": [
    "# Reducing data set size\n",
    "# Check the current size of the dataset\n",
    "mem = transactions.memory_usage(index=True).sum()\n",
    "print(mem/ 1024**2,\" MB\")\n",
    "\n",
    "def change_datatype(df):\n",
    "    '''This function will reduce the data type of each column to the minimum size available given the values'''\n",
    "    int_cols = list(df.select_dtypes(include='int64').columns)\n",
    "    for col in int_cols:\n",
    "        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):\n",
    "            df[col] = df[col].astype(np.int8)\n",
    "        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):\n",
    "            df[col] = df[col].astype(np.int16)\n",
    "        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "        else:\n",
    "            df[col] = df[col].astype(np.int64)\n",
    "            \n",
    "    float_cols = list(df.select_dtypes(include='float').columns)\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Apply the change_datatype function to the transactions data and check the change in memory used\n",
    "transactions = change_datatype(transactions)\n",
    "mem = transactions.memory_usage(index=True).sum()\n",
    "print(mem/ 1024**2,\" MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate whether any discounts have been applied to users trasnactions\n",
    "transactions['discount'] = (transactions['plan_list_price'] - transactions['actual_amount_paid'])/transactions['plan_list_price']\n",
    "transactions['is_discount'] = transactions.discount.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Reformate the date columns as dates\n",
    "transactions['transaction_date'] = pd.to_datetime(transactions['transaction_date'], format='%Y%m%d')\n",
    "transactions['membership_expire_date'] = pd.to_datetime(transactions['membership_expire_date'], format='%Y%m%d')\n",
    "\n",
    "transactions = change_datatype(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "953.3426291942596\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Groups transactions by msno (user id), and performs a variety of calculations given users prior transactions\n",
    "\n",
    "transactions_v2 = transactions.groupby('msno').agg(transaction_count=('payment_method_id', 'count'), # Number of transactions\n",
    "                        total_payment_plan_days= ('payment_plan_days','sum'), # Total number of payment plan days\n",
    "                        avg_payment_plan_days= ('payment_plan_days','mean'), # Avg payment plan days per transaction\n",
    "                        membership_expire_date = ('membership_expire_date', 'max'), # Last expiry date of membership\n",
    "                        most_recent_transaction = ('transaction_date', 'max'), # Most recent transaction date\n",
    "                        first_payment_plan = ('transaction_date', 'min'),  # First transaction date\n",
    "                        plan_net_worth=('plan_list_price', 'sum'), # The total list price of the users plan history\n",
    "                        mean_payment_each_transaction = ('actual_amount_paid', 'mean'), # Mean amount paid per transaction\n",
    "                        total_actual_payment = ('actual_amount_paid', 'sum'), # The total amount paid over the users account history\n",
    "                        normal_payment_method_id = ('payment_method_id', lambda x:x.value_counts().index[0]), # Most frequently used payment method\n",
    "                        auto_renew_times = ('is_auto_renew', lambda x : sum(x==1)), # Number of times auto renewed\n",
    "                        cancel_times = ('is_cancel', lambda x : sum(x==1)), # Number of times the user has historically cancelled\n",
    "                        average_discount= ('discount', 'mean'), # Average discount applied across transaction history\n",
    "                        biggest_discount= ('discount', 'max'), # Biggest discount recieved over all transactions\n",
    "                        discount_times= ('is_discount',  lambda x : sum(x==1)) # Number of times discount has been recieved\n",
    "                                                  )\n",
    "\n",
    "end = time.time()\n",
    "del transactions\n",
    "print(end-start)\n",
    "\n",
    "transactions_v2['amt_per_day'] = transactions_v2['total_actual_payment'] / transactions_v2['total_payment_plan_days']\n",
    "\n",
    "transactions_v2.reset_index(inplace=True)\n",
    "transactions_v2 = change_datatype(transactions_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groups user logs by msno (user id) o calculate streaming information for users for the month preceeding the churn prediction\n",
    "\n",
    "user_logs_v2 = user_logs.groupby('msno').agg(first_listen_date =('date', 'min'), # First listen date in the period\n",
    "                                most_recent_listen_date =('date', 'max'), # Last listen date in the period\n",
    "                                number_days_listened = ('date', 'count'), # Number of days listened in the period\n",
    "                                num_25 = ('num_25', 'sum'), # of songs played less than 25% of the song length\n",
    "                                num_50 = ('num_50', 'sum'), # of songs played between 25% to 50% of the song length\n",
    "                                num_75 = ('num_75', 'sum'), # of songs played between 50% to 75% of of the song length\n",
    "                                num_985 = ('num_985', 'sum'), # of songs played between 75% to 98.5% of the song length\n",
    "                                num_100 = ('num_100', 'sum'), # of songs played over 98.5% of the song length\n",
    "                                num_unq = ('num_unq', 'sum'), # Total number of daily unique songs played\n",
    "                                total_secs = ('total_secs', 'sum')) # Total number of seconds listened\n",
    "\n",
    "del user_logs\n",
    "\n",
    "# Calculate the total number of song plays in the listening period\n",
    "user_logs_v2[\"total_plays\"] = user_logs_v2['num_25']+user_logs_v2['num_50']+user_logs_v2['num_75']+user_logs_v2['num_985']+user_logs_v2['num_100']\n",
    "\n",
    "# Calculate the percentage of number of songs played within certain period\n",
    "user_logs_v2['percent_25'] = user_logs_v2['num_25']/(user_logs_v2['total_plays'])\n",
    "user_logs_v2['percent_50'] = user_logs_v2['num_50']/(user_logs_v2['total_plays'])\n",
    "user_logs_v2['percent_100'] = (user_logs_v2['num_985']+user_logs_v2['num_100'])/(user_logs_v2['total_plays'])\n",
    "\n",
    "# Calcualte metrics based on average X per day\n",
    "user_logs_v2[\"avg_seconds_per_day\"] = user_logs_v2[\"total_secs\"]/user_logs_v2[\"number_days_listened\"]\n",
    "user_logs_v2[\"avg_plays_per_day\"] = user_logs_v2[\"total_plays\"]/user_logs_v2[\"number_days_listened\"]\n",
    "user_logs_v2[\"avg_unique_per_day\"] = user_logs_v2[\"num_unq\"]/user_logs_v2[\"number_days_listened\"]\n",
    "\n",
    "# Convert date columns\n",
    "user_logs_v2['first_listen_date'] = pd.to_datetime(user_logs_v2['first_listen_date'], format = '%Y%m%d')\n",
    "user_logs_v2['most_recent_listen_date'] = pd.to_datetime(user_logs_v2['most_recent_listen_date'], format = '%Y%m%d')\n",
    "\n",
    "# drop unused variables\n",
    "user_logs_v2 = user_logs_v2.drop([\"num_unq\"], axis = 1)\n",
    "user_logs_v2.reset_index(inplace = True)\n",
    "user_logs_v2 = change_datatype(user_logs_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the aggregated user information between different tables to the churn data (train)\n",
    "dataset_train = train.merge(members, on = 'msno', how = 'left').merge(transactions_v2, on = 'msno', how = 'left').merge(user_logs_v2, on = 'msno', how = 'left')\n",
    "\n",
    "# Reduce space requirements\n",
    "del transactions_v2, user_logs_v2\n",
    "\n",
    "# Convert the date columns to date\n",
    "dataset_train['registration_init_time'] = pd.to_datetime(dataset_train['registration_init_time'], format = '%Y%m%d')\n",
    "\n",
    "# Create new date diff columns for modelling purpose:\n",
    "# Days between membership expirey and the latest listen\n",
    "dataset_train['day_diff_membership_expire__last_listen'] = (dataset_train['membership_expire_date'] - dataset_train['most_recent_listen_date']).astype('timedelta64[D]')\n",
    "# The days between the most recent listen and the first listen (within the period)\n",
    "dataset_train['day_diff_last_listen__first_listen'] = (dataset_train['most_recent_listen_date'] - dataset_train['first_listen_date']).astype('timedelta64[D]')\n",
    "# Day difference between the membership expiry and the initial registration (customer tenure)\n",
    "dataset_train['day_diff_membership_expire__registration'] = (dataset_train['membership_expire_date'] - dataset_train['registration_init_time']).astype('timedelta64[D]')\n",
    "# Day difference between membership expirey and the day before the churn calculation\n",
    "dataset_train['day_diff_membership_expire__registration'] = (dataset_train['membership_expire_date'] - dataset_train.most_recent_listen_date.max()).astype('timedelta64[D]')\n",
    "\n",
    "# This feature was removed due to the data leakage meaning the model was \"cheating\"\n",
    "#dataset_train['day_diff_membership_expire__last_transaction'] = (dataset_train['membership_expire_date'] - dataset_train['most_recent_transaction']).astype('timedelta64[D]')\n",
    "\n",
    "# Fix other columns\n",
    "# bd: Age (years), messy column, replaced values that are outwith the expected range with Nulls\n",
    "dataset_train['bd'] = np.where((dataset_train['bd'] >100), None,dataset_train['bd'])\n",
    "dataset_train['bd'] = np.where((dataset_train['bd'] <5), None,dataset_train['bd'])\n",
    "\n",
    "# Flag for if the user has no listening data during the period\n",
    "dataset_train[\"no_listen_flag\"] = dataset_train.first_listen_date.isnull().astype(int)\n",
    "# Flag for if the user has no transaction history (this shouldnt not happen but is added due to data quality issues)\n",
    "dataset_train[\"no_transactions_flag\"] = dataset_train.transaction_count.isnull().astype(int)\n",
    "\n",
    "# remove unsured columns\n",
    "dataset_train_v2 = dataset_train.drop(columns = ['msno', 'registration_init_time', 'most_recent_transaction', 'membership_expire_date', 'most_recent_listen_date', 'first_listen_date', 'first_payment_plan'])\n",
    "del dataset_train\n",
    "# Reset data types to minimise space requirement\n",
    "dataset_train_v2 = change_datatype(dataset_train_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-a828b365fddf>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[i][df[i] < lower] = lower\n",
      "<ipython-input-7-a828b365fddf>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[i][df[i] > upper] = upper\n"
     ]
    }
   ],
   "source": [
    "# Define a functiona that replaces outliers. \n",
    "def replaceoutlier(df, cols):\n",
    "    '''This function replaces outliers given a list of columns. \n",
    "    An outlier is defined as a value which is greater than the mean +- 3x standard deviation\n",
    "    The replacement value is the mean +- 3x standard deviation'''\n",
    "    for i in cols:\n",
    "        mean, std = np.mean(dataset_train_v2[i]), np.std(dataset_train_v2[i])\n",
    "        cut_off = std*3\n",
    "        lower, upper = mean - cut_off, mean + cut_off\n",
    "        df[i][df[i] < lower] = lower\n",
    "        df[i][df[i] > upper] = upper\n",
    "    return df\n",
    "\n",
    "# Define all columns where outliers should be replaced\n",
    "outlier_replace_cols = ['total_payment_plan_days', 'avg_payment_plan_days',\n",
    "       'plan_net_worth', 'mean_payment_each_transaction',\n",
    "       'total_actual_payment', 'average_discount', 'biggest_discount',\n",
    "       'discount_times', 'amt_per_day', 'number_days_listened', 'num_25',\n",
    "       'num_50', 'num_75', 'num_985', 'num_100', 'total_secs',\n",
    "       'total_plays', 'percent_25', 'percent_50', 'percent_100',\n",
    "       'avg_seconds_per_day', 'avg_plays_per_day', 'avg_unique_per_day',\n",
    "       'day_diff_membership_expire__last_listen',\n",
    "       'day_diff_last_listen__first_listen',\n",
    "       'day_diff_membership_expire__registration']\n",
    "\n",
    "# Apply the outlier replacement function on the data\n",
    "dataset_train_v2 = replaceoutlier(dataset_train_v2, outlier_replace_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns in which we are replacing NA's, along with the value which they should be replaced with\n",
    "fill_na_cols = {'transaction_count':0, 'number_days_listened':0, 'num_25':0,\n",
    "       'num_50':0, 'num_75':0, 'num_985':0, 'num_100':0, 'total_secs':0,\n",
    "       'total_plays':0, 'avg_seconds_per_day':0, 'avg_plays_per_day':0, 'avg_unique_per_day':0}\n",
    "\n",
    "dataset_train_v2 = dataset_train_v2.fillna(fill_na_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here categorical variables with high cardinality are replaced with the mean of the target variable\n",
    "# As we have not seperated test and train this will result in a small amount of leakage, which we will concee\n",
    "\n",
    "# Define the columns where we will replace the categorical values with the target mean\n",
    "target_mean_cols = ['city', 'normal_payment_method_id']\n",
    "\n",
    "values = []\n",
    "for i in target_mean_cols:\n",
    "    # Take the mean value of the outcome variable (churn) for each categorical value\n",
    "    df = dataset_train_v2.groupby(i)['is_churn'].mean().reset_index()\n",
    "    df[\"feature\"] = i\n",
    "    list_means = df.to_numpy().tolist()\n",
    "    # Create a list with the means and the categorical values\n",
    "    for means_item in list_means:\n",
    "        values.append(means_item)\n",
    "\n",
    "# Create a data frame of mean values for each feature and value\n",
    "means_df = pd.DataFrame(values, columns=['value', 'mean', 'feature'])\n",
    "\n",
    "# Create new columns with the mean target mean encdings, and drop the old columns\n",
    "for i in target_mean_cols:\n",
    "    means = means_df[means_df.feature == i]\n",
    "    means=means.rename(columns = {\"value\": i, \"mean\": i+\"_mean\"}).drop([\"feature\"], axis = 1)\n",
    "    dataset_train_v2 = dataset_train_v2.merge(means, on = i, how = 'left')\n",
    "    dataset_train_v2=dataset_train_v2.drop(i, axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical values with low cardinality are one hot coded\n",
    "ohe_cols = ['gender', 'registered_via']\n",
    "dataset_train_v2 = pd.get_dummies(data = dataset_train_v2,columns = ohe_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the churn column\n",
    "dataset_train_v2=dataset_train_v2.rename(columns={\"is_churn\": \"Churn\"})\n",
    "\n",
    "# Write the prepared data to the output location\n",
    "path_new = \"C:/Users/andrew.davidson/OneDrive - Concentra Consulting Limited/Documents/Projects/GBT Feature Relationships/KK Box/data/\"\n",
    "dataset_train_v2.to_csv(path_new + \"Prepped Data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
